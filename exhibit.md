# Selected works

1. [Champernown: "Music from EDSAC" (circa 1960)](#d-g-champernown-music-from-edsac-circa-1960)
1. [Ben-Tal: "Notes for a future self"](#oded-ben-tal-notes-for-a-future-self)
1. [Laidlow: "Alter" for mezzo-sporano and ensemble](#robert-laidlow-alter-for-mezzo-sporano-and-ensemble)
1. [Collaborative Electroacoustic Composition with Intelligent Agents](#collaborative-electroacoustic-composition-with-intelligent-agents-cecia)
1. [Kokoras: "AI Phantasy"](#panayiotis-kokoras-ai-phantasy)
1. [Coelho: "Music Transformer and DDSP Étude of Composition and Digital Performances"](#guilherme-coelho-music-transformer-and-ddsp-etude-of-composition-and-digital-performances)
1. [Hayes: "Moon via spirit" for live electronics](#lauren-hayes-moon-via-spirit-for-live-electronics)
1. [Lopez: "The Journey"](#alvaro-lopez-the-journey)
1. [Frisk: "pvm"](#henrik-frisk-pvm)

---

#### D. G. Champernown "Music from EDSAC" (circa 1960)

This string quartet arose from two computer programmes written by David G. Champernown, Professor of Economics at Cambridge University, around 1960. One programme harmonized melodies in the style of Victorian hymns. Another program generated serial-style music. A performance and recording of the string quartet was supposedly made by Lejaren A. Hiller in the 1960s, but this has been lost. <a href="https://highnoongmt.wordpress.com/2020/10/01/music-from-edsac-circa-1960/" target="_blank">See here for more information</a>.

<a href="https://www.youtube.com/watch?v=gogIM2kKB1U&list=PLT_o2wa6T9d6ZMPnYW13XS6UoqymtxzN4
" target="_blank"><img src="http://img.youtube.com/vi/gogIM2kKB1U/0.jpg" 
alt="IMAGE ALT TEXT HERE" width="240" height="180" border="10" /></a>

[Go to menu](#selected-works)
---

#### Oded Ben-Tal: "Notes for a future self"

This work was composed using deep learning tools, specifically [folk-rnn](https://github.com/IraKorshunova/folk-rnn) and [Magenta](https://magenta.tensorflow.org/). Both are deep learning models of symbolic music which I used in the composition process to generate material that I than transformed and adapted. Most of the material given to the percussionists came out of Magenta models. I experimented with both the melody generation and drum pattern generation modes and discovered that the most interesting results came out of confusing those: seeding the model with a melodic fragment but setting it to generate drum patterns. Or the other way round. The sequences generated in these ways were further transformed by mapping them onto groups of percussion instruments. The player is provided with guidelines about the composition of the set but is free to construct their own percussion set. Both the flute and the clarinet have extended solo moments in the piece, the melodic material of which was generated by folkrnn and further extended with Magenta. This material was again transformed in the composition away from the folk idiom of the origin. 

This is the third piece I composed using machine learning tools - following [Bastard Tunes (2017)](https://www.youtube.com/playlist?list=PLdTpPwVfxuXpQ03F398HH463SAE0vR2X8) and [Between the Lines (2018)](https://youtu.be/GdvylH-0Q1k). In each piece the interaction between my own creative ideas and the machine learning system is different, but some general themes are emerging. The default output of the system is mostly useless. The initial phase involves significant amount of learning, on my part, of their supposed learning. Interesting results starts to appear when I learn how to subvert the model away from the training set. Starting the co-creative process with strong ideas about what I want to get out of the model is not going to work. If I know what I need I should either create it or program it.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=QmYt46Wl8JY
" target="_blank"><img src="http://img.youtube.com/vi/QmYt46Wl8JY/0.jpg" 
alt="IMAGE ALT TEXT HERE" width="240" height="180" border="10" /></a>

Oded Ben-Tal is 

[Go to menu](#selected-works)
---

#### Robert Laidlow: Alter for mezzo-sporano and ensemble

---

#### Collaborative Electroacoustic Composition with Intelligent Agents (CECIA)

---

#### Panayiotis Kokoras: AI Phantasy

---

#### Guilherme Coelho: Music Transformer and DDSP Étude of Composition and Digital Performances

---

#### Lauren Hayes: Moon via spirit for live electronics

This piece was commissioned as part of the Fluid Corpus Manipulation (FluCoMa) project, from the University of Huddersfield. The project studies how creative coders and technologists work with and incorporate new digital tools for signal decomposition and machine learning in novel ways. In this piece, I explore these tools through an embodied approach to segmentation, slicing, and layering of sound in real time. Using the FluCoMa toolkit, I was able to incorporate novel machine learning techniques in MaxMSP which deal with exploring large corpora of sound files. Specifically, this work involves, among other relevant AI techniques, machine learning in order to train based on preference; sort and select based on descriptors; and concatenate percussion sounds from a large collection of samples.

[Lauren Hayes](https://www.pariesa.com/) is a Scottish musician and sound artist who builds hybrid analogue/digital instruments and unpredictable performance systems. As an improviser, her music has been described as 'voracious' and ‘exhilarating’. Her research explores embodied music cognition, enactive approaches to digital instrument design, and haptic technologies. She is currently Assistant Professor of Sound Studies within the School of Arts, Media and Engineering at Arizona State University where she leads PARIESA (Practice and Research in Enactive Sonic Art). She is Director-At-Large of the International Computer Music Association and is a member of the New BBC Radiophonic Workshop.

---

#### Alvaro Lopez: The Journey

The Journey is a piece resulting from Alvaro E. Lopez’s real-time performance on AMG (Algorithmic Music Generator), a Max patch that generates music adaptively. It illustrates several techniques described in the article [Lopez, "Algorithmic Interactive Music Generation in Videogames", SoundEffecs 9(1), 2020](https://www.soundeffects.dk/article/view/118245).

---

#### Henrik Frisk: pvm

